# Firecracker VM IPv6 Routing Implementation Guide

## Architecture Overview

Every Firecracker VM gets a unique IPv6 address from your datacenter allocation. Edge proxies route traffic directly to these VMs using a Postgres-backed service discovery system.

## Component Design

### 1. IPv6 Address Allocation Strategy

**Hierarchical allocation scheme:**

- Divide your IPv6 space hierarchically: Region → Node → VM
- Example: `2001:db8:REGION:NODE:VM::1/128`
- Each region gets a /48, each node gets a /64, each VM gets a /128
- This makes routing aggregation simple and debugging easier

**Benefits:**

- Just by looking at an IP, you know exactly which region/node hosts the VM
- BGP advertisements can be aggregated at node level
- No IP allocation conflicts between nodes

### 2. Database Schema Design

**Core tables needed in PlanetScale Postgres:**

**Deployments table:**

- Stores customer deployments (deployment_id, customer_id, domain, configuration)
- Domain is unique and used for routing lookup

**VMs table:**

- Records every VM instance (vm_id, deployment_id, node_id, ipv6_address, status, health)
- Status tracks lifecycle: starting → running → draining → stopped
- Health tracked separately: healthy/unhealthy/unknown

**Routing table (denormalized cache):**

- Pre-computed mapping of domain → list of healthy VMs
- Updated whenever VM health/status changes
- Includes version number for cache invalidation
- Edge proxies poll this table instead of doing complex joins

### 3. Node Network Configuration

**Each physical node needs:**

- IPv6 forwarding enabled in kernel
- Route for its /64 prefix pointing to local interface
- BPF or iptables rules to forward packets to VM TAP interfaces
- Neighbor discovery responses for VM IPs

**Per-VM networking:**

- Create TAP interface for each VM
- Add host route: VM's /128 → TAP interface
- Configure Firecracker to use this TAP
- VM gets static IPv6 configuration via kernel boot args or cloud-init

### 4. VM Lifecycle Management

**VM Creation flow:**

1. Orchestrator picks a node with capacity
2. Allocates next available IPv6 from node's range
3. Creates TAP interface and routing rules
4. Starts Firecracker process with TAP attached
5. Inserts VM record into database with status='starting'
6. Waits for VM to boot and pass health check
7. Updates status to 'running' and health to 'healthy'
8. Triggers routing table rebuild for this deployment

**VM Deletion flow:**

1. Set VM status to 'draining' in database
2. Routing table update removes VM from rotation
3. Wait 30 seconds for existing connections to complete
4. Stop Firecracker process
5. Clean up TAP interface and routes
6. Mark VM as 'stopped' in database

### 5. Edge Proxy Routing Logic

**Request handling flow:**

1. Extract Host header from incoming request (e.g., app.dokedu.org)
2. Look up domain in local routing cache
3. Get list of healthy VMs for this deployment
4. Select VM using load balancing algorithm (round-robin/least-connections)
5. Proxy HTTP request to `[ipv6]:8080`
6. If connection fails, mark VM unhealthy and retry with different VM

**Routing cache management:**

- Poll routing_table every 5 seconds for updates
- Compare version numbers to detect changes
- Keep in-memory map: domain → VM list
- Maintain round-robin index per deployment

**Connection pooling:**

- Maintain persistent HTTP/2 connections to each VM
- Pre-establish connections to new VMs before adding to rotation
- Gracefully drain connections to unhealthy VMs

### 6. Health Checking System

**Health checker service (runs on each edge proxy or centrally):**

- Query database for all running VMs every 10 seconds
- Send parallel HTTP requests to `/health` endpoint on each VM
- Batch update health status in database
- Trigger routing table rebuild for affected deployments

**Health check states:**

- Healthy: VM responding with 200 OK
- Unhealthy: VM not responding or returning errors
- Unknown: New VM not yet checked

**Flap prevention:**

- Require 2 consecutive failures before marking unhealthy
- Require 2 consecutive successes before marking healthy again
- Add exponential backoff for chronically unhealthy VMs

### 7. Service Discovery Flow

**How components stay in sync:**

1. **VM Manager** (on each node):

   - Writes VM records when creating/destroying VMs
   - Updates VM status during lifecycle changes

2. **Health Checker**:

   - Updates health_status field based on health checks
   - Triggers routing table rebuilds via database trigger or explicit update

3. **Routing Table Builder** (can be trigger-based or separate service):

   - Monitors VMs table for changes
   - Rebuilds routing_table entries for affected deployments
   - Increments version number on each update

4. **Edge Proxies**:
   - Poll routing_table for their region
   - Update local cache when version changes
   - Use cached routes for request handling

### 8. Handling Failures

**VM failure:**

- Health checker detects and marks unhealthy
- Edge proxy routes around failed VM
- Orchestrator eventually replaces failed VM

**Node failure:**

- All VMs on node become unreachable
- Health checker marks all as unhealthy
- Orchestrator recreates VMs on other nodes
- BGP withdraws routes for failed node

**Edge proxy failure:**

- L4 load balancer detects and routes around
- Other edge proxies handle traffic
- No impact on routing table

**Database failure:**

- Edge proxies continue using cached routing table
- New VMs can't be added until database recovers
- Consider read replicas for edge proxy queries

### 9. Optimizations

**Performance improvements:**

- Use prepared statements for all database queries
- Batch VM updates instead of individual updates
- Add database indexes on (deployment_id, status, health_status)
- Consider partitioning VMs table by region

**Reduce database load:**

- Edge proxies only poll routing_table, never VMs directly
- Use connection pooling in all database clients
- Consider caching layer (Redis) if database becomes bottleneck

**Network optimizations:**

- Use BPF for packet forwarding instead of iptables
- Enable TCP BBR congestion control
- Use jumbo frames in datacenter network
- Consider ECMP for load distribution

### 10. Monitoring & Observability

**Key metrics to track:**

- VMs per node/region
- Health check success rate
- Routing table update latency
- Edge proxy cache hit rate
- P50/P99 proxy latency
- Database query performance

**Debugging tools needed:**

- Trace request from edge proxy to specific VM
- Show all VMs for a deployment with health status
- Display routing table history for a domain
- Per-VM traffic and error rates

## Implementation Phases

**Phase 1: Basic routing**

- Database schema setup
- Simple VM creation with IPv6
- Manual routing table updates
- Single edge proxy implementation

**Phase 2: Health checking**

- Automated health checker service
- Dynamic routing table updates
- Multiple edge proxy support

**Phase 3: Production hardening**

- Connection draining
- Graceful shutdown
- Monitoring and alerting
- Performance optimization

**Phase 4: Scale testing**

- Load test with 1000+ VMs
- Chaos testing (kill nodes/VMs)
- Database performance tuning
- Edge proxy autoscaling

This approach provides a solid foundation that can scale to millions of VMs while keeping the complexity manageable.
